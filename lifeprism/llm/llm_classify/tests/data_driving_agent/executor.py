# æ‰§è¡Œå™¨å®šä¹‰
from lifeprism.llm.llm_classify.tests.data_driving_agent.schemas import Context, NodeDefinition, ExecutionPlan
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from lifeprism.llm.llm_classify.utils import create_ChatTongyiModel
from lifeprism.llm.llm_classify.tools.database_tools import (
    get_daily_stats,
    get_multi_days_stats,
    query_behavior_logs,
    query_goals,
    query_psychological_assessment
)


class Executor:
    # é»˜è®¤å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶
    DEFAULT_TOOLS_USAGE_LIMIT = {
        "get_daily_stats": 1,
        "get_multi_days_stats": 1,
        "query_behavior_logs": 10,
        "query_goals": 1,
        "query_psychological_assessment": 1
    }

    def __init__(self, plan: ExecutionPlan, user_message: str, tools_limit: dict[str, int] | None = None):
        self.plan = plan
        self.context = Context(messages=[HumanMessage(content=user_message)])
        self.tools_map = {
            "get_daily_stats": get_daily_stats,
            "get_multi_days_stats": get_multi_days_stats,
            "query_behavior_logs": query_behavior_logs,
            "query_goals": query_goals,
            "query_psychological_assessment": query_psychological_assessment
        }
        # ä¿å­˜åˆå§‹å·¥å…·é™åˆ¶é…ç½®ï¼Œç”¨äº reset
        # å…ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼Œå†ç”¨ tools_limit ä¸­çš„å€¼è¦†ç›–
        self._initial_tools_limit = self.DEFAULT_TOOLS_USAGE_LIMIT.copy()
        if tools_limit:
            self._initial_tools_limit.update(tools_limit)
        self.tools_usage_limit = self._initial_tools_limit.copy()
        # tokens ä½¿ç”¨ç»Ÿè®¡
        self.tokens_usage = {
            'input_tokens': 0,
            'output_tokens': 0,
            'total_tokens': 0
        }

    def reset_tools_limit(self):
        """é‡ç½®å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶ä¸ºåˆå§‹é…ç½®"""
        self.tools_usage_limit = self._initial_tools_limit.copy()
    
    def reset_tokens_usage(self):
        """é‡ç½® tokens ä½¿ç”¨ç»Ÿè®¡"""
        self.tokens_usage = {
            'input_tokens': 0,
            'output_tokens': 0,
            'total_tokens': 0
        }
    
    def _accumulate_tokens(self, result) -> None:
        """
        ç´¯åŠ  tokens ä½¿ç”¨é‡
        
        args:
            result: LLM è¿”å›çš„ç»“æœå¯¹è±¡
        """
        if hasattr(result, 'response_metadata') and 'token_usage' in result.response_metadata:
            token_usage = result.response_metadata['token_usage']
            self.tokens_usage['input_tokens'] += token_usage.get('input_tokens', 0)
            self.tokens_usage['output_tokens'] += token_usage.get('output_tokens', 0)
            self.tokens_usage['total_tokens'] += token_usage.get('total_tokens', 0)
    
    def get_history(self) -> str:
        """è¿”å›æ ¼å¼åŒ–çš„å†å²æ¶ˆæ¯å­—ç¬¦ä¸²"""
        result = []
        for message in self.context["messages"]:
            if isinstance(message, HumanMessage):
                result.append(f"user: {message.content}")
            elif isinstance(message, ToolMessage):
                result.append(f"tool: {message.content}")
            elif isinstance(message, AIMessage):
                result.append(f"assistant: {message.content}")
        return "\n".join(result) if result else ""

    def _create_llm_with_tools(self, tools: list[str] | None):
        """åˆ›å»º LLMï¼Œå¦‚æœæœ‰å·¥å…·åˆ™ç»‘å®š"""
        llm = create_ChatTongyiModel(enable_search=False, enable_thinking=False)
        if tools:
            # è·å–å·¥å…·å¯¹è±¡
            tool_objects = [self.tools_map[t] for t in tools]
            llm = llm.bind_tools(tool_objects)
        return llm
    
    def _validate_tools(self, tools: list[str] | None):
        """éªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨"""
        if not tools:
            return
        for tool in tools:
            if tool not in self.tools_map:
                raise ValueError(f"å·¥å…· {tool} ä¸å­˜åœ¨ï¼Œå¯ç”¨å·¥å…·: {list(self.tools_map.keys())}")
    
    def _tools_limit_prompt(self, tools: list[str] | None) -> str:
        """
        ç”Ÿæˆå·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶çš„ prompt
        
        args : 
            tools : list[str] è°ƒç”¨çš„å·¥å…·åç§°åˆ—è¡¨
        return : 
            str : å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶çš„ prompt
        """
        if not tools:
            return ""
        
        lines = []
        for tool in tools:
            remaining = self.tools_usage_limit.get(tool, 0)
            lines.append(f"å·¥å…· {tool} å¯ä»¥è°ƒç”¨ {remaining} æ¬¡")
        return "\n".join(lines)
    
    def _can_use_tool(self, tool_name: str) -> bool:
        """
        åˆ¤æ–­æŒ‡å®šå·¥å…·æ˜¯å¦è¿˜èƒ½è°ƒç”¨
        
        args:
            tool_name: å·¥å…·åç§°
        return:
            bool: æ˜¯å¦å¯ä»¥è°ƒç”¨
        """
        return self.tools_usage_limit.get(tool_name, 0) > 0
    
    def _consume_tool_usage(self, tool_name: str) -> None:
        """
        æ¶ˆè€—ä¸€æ¬¡å·¥å…·è°ƒç”¨æ¬¡æ•°
        
        args:
            tool_name: å·¥å…·åç§°
        """
        if tool_name in self.tools_usage_limit:
            self.tools_usage_limit[tool_name] -= 1

    def _get_prompt(self, node: NodeDefinition) -> str:
        """æ„å»ºèŠ‚ç‚¹çš„ prompt"""
        tools_limit_prompt = self._tools_limit_prompt(node.tools)

        return f"""
# å†å²æ¶ˆæ¯
{self.get_history()}
# å·¥å…·å¯è°ƒç”¨æ¬¡æ•°é™åˆ¶ï¼Œè¯·åˆç†å®‰æ’å·¥å…·è°ƒç”¨:
{tools_limit_prompt}
# ä½ éœ€è¦æŒ‰ç…§ä¸‹é¢è¦æ±‚å®Œæˆä»»åŠ¡ï¼š
{node.task_prompt}
"""

    def _execute_tool_call(self, tool_call: dict) -> tuple[bool, str | None]:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨
        
        args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ŒåŒ…å« name, args, id
        return:
            tuple[bool, str | None]: (æ˜¯å¦æ‰§è¡ŒæˆåŠŸ, å·¥å…·è¿”å›ç»“æœæˆ–é”™è¯¯ä¿¡æ¯)
        """
        tool_name = tool_call.get("name", "")
        tool_args = tool_call.get("args", {})
        tool_id = tool_call.get("id", "")
        
        # éªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨
        if tool_name not in self.tools_map:
            error_msg = f"æœªçŸ¥å·¥å…·: {tool_name}ï¼Œå¯ç”¨å·¥å…·: {list(self.tools_map.keys())}"
            print(f"    âœ— {error_msg}")
            return False, error_msg
        
        # æ£€æŸ¥å·¥å…·æ˜¯å¦è¿˜æœ‰è°ƒç”¨æ¬¡æ•°
        if not self._can_use_tool(tool_name):
            error_msg = f"å·¥å…· {tool_name} è°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œ"
            print(f"    âœ— {error_msg}")
            # æ·»åŠ é”™è¯¯ä¿¡æ¯åˆ° context
            self.context["messages"].append(ToolMessage(
                content=error_msg,
                tool_call_id=tool_id
            ))
            return False, error_msg
        
        # æ‰§è¡Œå·¥å…·
        print(f"    - æ‰§è¡Œå·¥å…·: {tool_name}, args: {tool_args}")
        tool_result = self.tools_map[tool_name].invoke(tool_args)
        
        # æ¶ˆè€—è°ƒç”¨æ¬¡æ•°
        self._consume_tool_usage(tool_name)
        print(f"    - å·¥å…· {tool_name} å‰©ä½™è°ƒç”¨æ¬¡æ•°: {self.tools_usage_limit[tool_name]}")
        
        # æ·»åŠ  ToolMessage åˆ° context
        self.context["messages"].append(ToolMessage(
            content=str(tool_result),
            tool_call_id=tool_id
        ))
        
        return True, str(tool_result)
    
    def _has_available_tools(self, tools: list[str] | None) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ç”¨çš„å·¥å…·è°ƒç”¨æ¬¡æ•°
        
        args:
            tools: å½“å‰èŠ‚ç‚¹å¯ç”¨çš„å·¥å…·åˆ—è¡¨
        return:
            bool: æ˜¯å¦è¿˜æœ‰å¯ç”¨çš„å·¥å…·
        """
        if not tools:
            return False
        return any(self._can_use_tool(tool) for tool in tools)

    def _execute_node(self, node: NodeDefinition):
        """
        æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨å¾ªç¯
        
        å¾ªç¯é€»è¾‘ï¼š
        1. è°ƒç”¨ LLM
        2. å¦‚æœè¿”å› tool_calls â†’ æ‰§è¡Œå·¥å…· â†’ æ·»åŠ  ToolMessage â†’ å›åˆ°æ­¥éª¤ 1
        3. å¦‚æœè¿”å›çº¯æ–‡æœ¬ â†’ ç»“æŸå¾ªç¯
        4. å¦‚æœæ‰€æœ‰å·¥å…·è°ƒç”¨æ¬¡æ•°ç”¨å®Œ â†’ ç»“æŸå¾ªç¯
        """
        print(f"æ‰§è¡ŒèŠ‚ç‚¹ï¼š{node.node_name}")
        
        # éªŒè¯å·¥å…·
        self._validate_tools(node.tools)
        
        # åˆ›å»º LLMï¼ˆå¸¦å·¥å…·ï¼‰
        llm = self._create_llm_with_tools(node.tools)
        result = None
        
        # â­ å·¥å…·è°ƒç”¨å¾ªç¯ â­
        while True:
            # æ„å»º prompt
            prompt = self._get_prompt(node)
            
            # 1. è°ƒç”¨ LLM
            result = llm.invoke(prompt)
            
            # 2. ç´¯åŠ  tokens ä½¿ç”¨é‡
            self._accumulate_tokens(result)
            
            # 3. æ·»åŠ  AIMessage åˆ° context
            self.context["messages"].append(result)
            
            # 3. æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
            if not (hasattr(result, 'tool_calls') and result.tool_calls):
                print(f"  â†’ LLM è¿”å›æœ€ç»ˆç»“æœ")
                break
            
            # 4. æ‰§è¡Œå·¥å…·è°ƒç”¨
            print(f"  â†’ LLM è¯·æ±‚è°ƒç”¨ {len(result.tool_calls)} ä¸ªå·¥å…·")
            
            executed_count = 0
            for tool_call in result.tool_calls:
                success, _ = self._execute_tool_call(tool_call)
                if success:
                    executed_count += 1
            
            # 5. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ç”¨çš„å·¥å…·
            if not self._has_available_tools(node.tools):
                print(f"  â†’ æ‰€æœ‰å·¥å…·è°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œ")
                break
            
            # 6. å¦‚æœæœ¬è½®æ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•å·¥å…·ï¼Œç»“æŸå¾ªç¯
            if executed_count == 0:
                print(f"  â†’ æœ¬è½®æ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•å·¥å…·")
                break
            
            # 7. ç»§ç»­å¾ªç¯ï¼Œè®© LLM çœ‹åˆ°å·¥å…·ç»“æœ
            print(f"  â†’ ç»§ç»­è°ƒç”¨ LLMï¼ŒæŸ¥çœ‹å·¥å…·ç»“æœ...")
        
        return result.content
    
    def execute(self):
        """
        æ‰§è¡Œæ•´ä¸ªè®¡åˆ’
        
        Returns:
            dict: åŒ…å«æ‰§è¡Œç»“æœçš„å­—å…¸
                - content: æœ€ç»ˆè¾“å‡ºå†…å®¹
                - messages: æ‰€æœ‰æ¶ˆæ¯åˆ—è¡¨
                - tokens_usage: tokens ä½¿ç”¨é‡ç»Ÿè®¡
                    - input_tokens: è¾“å…¥ token æ•°é‡
                    - output_tokens: è¾“å‡º token æ•°é‡
                    - total_tokens: æ€» token æ•°é‡
        """
        print(f"\nå¼€å§‹æ‰§è¡Œè®¡åˆ’: {self.plan.task}\n")

        # é‡ç½®å·¥å…·è°ƒç”¨æ¬¡æ•°å’Œ tokens ç»Ÿè®¡
        self.reset_tools_limit()
        self.reset_tokens_usage()

        content = None
        for node in self.plan.nodes:
            content = self._execute_node(node)
        
        print(f"\nè®¡åˆ’æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“Š Tokens ä½¿ç”¨ç»Ÿè®¡: è¾“å…¥={self.tokens_usage['input_tokens']}, è¾“å‡º={self.tokens_usage['output_tokens']}, æ€»è®¡={self.tokens_usage['total_tokens']}\n")
        
        return {
            "content": content,
            "messages": self.context["messages"],
            "tokens_usage": self.tokens_usage
        }

if __name__ == "__main__":
    from lifeprism.llm.llm_classify.tests.data_driving_agent.plans import get_daily_summary_plan
    plan,tools_limit = get_daily_summary_plan("2026-01-05",json_path=r"D:\desktop\è½¯ä»¶å¼€å‘\LifeWatch-AI\lifeprism\llm\llm_classify\tests\data_driving_agent\pattern\daily_summary_plan.json",pattern_name="simple")
    executor = Executor(plan, "æ€»ç»“ 2026-01-05 çš„ä½¿ç”¨æƒ…å†µ",tools_limit=tools_limit)
    result = executor.execute()
    
    # æ ¼å¼åŒ–è¾“å‡º
    print("\n" + "=" * 80)
    print("ğŸ“Š AI ç”Ÿæˆçš„è¡Œä¸ºæ€»ç»“")
    print("=" * 80 + "\n")
    print(result["content"])
    print("\n" + "=" * 80)
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ï¼šå…±äº§ç”Ÿ {len(result['messages'])} æ¡æ¶ˆæ¯")
    tokens = result["tokens_usage"]
    print(f"ğŸ”¢ Tokens ä½¿ç”¨: è¾“å…¥={tokens['input_tokens']}, è¾“å‡º={tokens['output_tokens']}, æ€»è®¡={tokens['total_tokens']}")
    print("=" * 80)


    
