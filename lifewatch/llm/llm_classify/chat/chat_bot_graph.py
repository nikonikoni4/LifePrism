"""
V2 ChatBot æ”¹ä¸ºä½¿ç”¨graph å¢åŠ åŠŸèƒ½è§£è¯´å’Œç›¸å…³åŠŸèƒ½è§£ç­”
"""
from lifewatch.llm.llm_classify.schemas.chatbot_schemas import ChatBotSchemas
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.checkpoint.memory import InMemorySaver
from typing import Optional, Union, AsyncGenerator, Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager
from lifewatch.llm.custom_prompt.common_prompt import intent_router_template,norm_chat_template
from lifewatch.llm.custom_prompt.chatbot_prompt.feature_introduce import intro_template,intro_router_template
from lifewatch.llm.llm_classify.utils import create_ChatTongyiModel
from langchain.tools import ToolRuntime
from typing import TypedDict
import json
from lifewatch.utils import get_logger
import logging
from langchain_core.messages import HumanMessage, AIMessage,AIMessageChunk
from langgraph.graph import StateGraph
from langgraph.types import RetryPolicy
logger = get_logger(__name__,logging.DEBUG)
class LLMParseError(Exception):
    """
    LLM è¾“å‡ºè§£æé”™è¯¯ - å¯é‡è¯•
    
    å½“ LLM è¿”å›çš„å†…å®¹æ— æ³•æ­£ç¡®è§£æï¼ˆå¦‚ JSON æ ¼å¼é”™è¯¯ã€ç¼ºå°‘å¿…è¦å­—æ®µç­‰ï¼‰æ—¶æŠ›å‡ºæ­¤é”™è¯¯ã€‚
    æ­¤é”™è¯¯ç±»å‹è¢«æ ‡è®°ä¸ºå¯é‡è¯•ï¼Œé‡è¯•æœºåˆ¶ä¼šæ•è·æ­¤é”™è¯¯å¹¶é‡æ–°è°ƒç”¨ LLMã€‚
    
    Attributes:
        message: é”™è¯¯æè¿°ä¿¡æ¯
        original_error: åŸå§‹å¼‚å¸¸ï¼ˆå¯é€‰ï¼‰
        raw_content: LLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
    """
    def __init__(self, message: str, original_error: Exception = None, raw_content: str = None):
        super().__init__(message)
        self.message = message
        self.original_error = original_error
        self.raw_content = raw_content

    def __str__(self):
        base_msg = f"LLMParseError: {self.message}"
        if self.raw_content:
            # æˆªæ–­è¿‡é•¿çš„åŸå§‹å†…å®¹
            content_preview = self.raw_content[:100] + "..." if len(self.raw_content) > 100 else self.raw_content
            base_msg += f"\nåŸå§‹å†…å®¹: {content_preview}"
        return base_msg


def get_history_messages(messages: list[HumanMessage| AIMessage]):
    history_messages = ""
    for i,msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            history_messages += f"{i}. User: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_messages += f"{i}. Assistant: {msg.content}\n"
    return history_messages

class ChatBot:
    def __init__(self,checkpointer: Optional[Union[InMemorySaver, AsyncSqliteSaver]] = None):
        self.current_total_tokens = 0
        self.tokens_usage = {}
        self.checkpointer = checkpointer or InMemorySaver()
        # ç”¨äºæµå¼è¾“å‡º
        self.llm_streaming = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=True,temperature=0.5)
        self.config: Optional[dict] = None
        self.thread_id = None
        # self._is_persistent = isinstance(self.checkpointer, AsyncSqliteSaver)
        # è¿™é‡Œçš„feature_listå¿…é¡»ä¸lifewatch\llm\custom_prompt\common_prompt.py
        # ä¸­çš„intent_router_templateä¸­çš„feature_listä¿æŒä¸€è‡´
        self.feature_list = ["lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£","ä¸€èˆ¬æ¨¡å¼"] # 
        self.graph = StateGraph(ChatBotSchemas)
        self.chatbot = self._build_graph()
    def _build_graph(self):
        """
        æ„å»ºå¯¹è¯æµç¨‹å›¾
        
        æµç¨‹ï¼š
        START â†’ intent_router â†’ (æ ¹æ®æ„å›¾åˆ†æ”¯)
            - "lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£" â†’ feat_intro_router â†’ feature_introduce â†’ END
            - å…¶ä»–æ„å›¾ â†’ norm_chat â†’ END
        """
        from langgraph.graph import START, END
        
        # æ·»åŠ èŠ‚ç‚¹
        self.graph.add_node("intent_router",
                            self.intent_router,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("feat_intro_router",
                            self.feat_intro_router,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("feature_introduce",
                            self.feature_introduce,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("norm_chat",
                            self.norm_chat,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError], max_attempts=2))
        
        # å®šä¹‰æ¡ä»¶è·¯ç”±å‡½æ•°
        def route_by_intent(state: ChatBotSchemas) -> str:
            """æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹"""
            intent = state.intent[-1] if state.intent else ""
            if intent == "lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£":
                return "feat_intro_router"
            else:
                return "norm_chat"
        
        # æ·»åŠ è¾¹
        # START â†’ intent_router
        self.graph.add_edge(START, "intent_router")
        
        # intent_router â†’ æ¡ä»¶åˆ†æ”¯
        self.graph.add_conditional_edges(
            "intent_router",
            route_by_intent,
            {
                "feat_intro_router": "feat_intro_router",
                "norm_chat": "norm_chat"
            }
        )
        
        # feat_intro_router â†’ feature_introduce
        self.graph.add_edge("feat_intro_router", "feature_introduce")
        
        # feature_introduce â†’ END
        self.graph.add_edge("feature_introduce", END)
        
        # norm_chat â†’ END
        self.graph.add_edge("norm_chat", END)
        
        # ç¼–è¯‘ graphï¼Œä¼ å…¥ checkpointer
        return self.graph.compile(checkpointer=self.checkpointer)


    def init_tokens_usage(self,thread_id:str):
        """
        åˆå§‹åŒ–æ–°ä¼šè¯çš„tokenä½¿ç”¨æƒ…å†µ
        """
        logger.debug(f"åˆå§‹åŒ–tokenä½¿ç”¨æƒ…å†µ: {thread_id}")
        if thread_id not in self.tokens_usage:
            self.tokens_usage[thread_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "search_count": 0
            }

    def set_thread_id(self, thread_id: str):
        """
        è®¾ç½®å½“å‰ä¼šè¯çš„ thread_idã€‚
        
        Args:
            thread_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯
        """
        logger.debug(f"è®¾ç½®thread_id: {thread_id}")
        self.config = {"configurable": {"thread_id": thread_id}}
        self.thread_id = thread_id
        self.init_tokens_usage(thread_id)
    def get_new_agent(self,enable_search:bool,enable_thinking:bool,enable_streaming:bool,temperature:float):
        """
        ç”¨äºè·å–æ–°çš„agent
        """
        logger.debug(f"è·å–æ–°çš„agent: enable_search={enable_search}, enable_thinking={enable_thinking}, enable_streaming={enable_streaming}, temperature={temperature}")
        return create_ChatTongyiModel(enable_search=enable_search,
                            enable_thinking=enable_thinking,
                            enable_streaming=enable_streaming,temperature=temperature)
    def update_usage(self,result):
        token_usage = result.response_metadata.get("token_usage", {})
        self.tokens_usage[self.thread_id]["input_tokens"] += token_usage.get("input_tokens", 0)
        self.tokens_usage[self.thread_id]["output_tokens"] += token_usage.get("output_tokens", 0)
        self.tokens_usage[self.thread_id]["total_tokens"] += token_usage.get("total_tokens", 0)
        # self.tokens_usage[self.thread_id]["call_count"] += 1 # è¿™é‡Œæœ‰é—®é¢˜ï¼Œæš‚æ—¶ä¸æ”¹

        

    @classmethod
    @asynccontextmanager
    async def create_with_persistence(
        cls,
        db_path: Union[str, Path] = r"lifewatch\llm\llm_classify\chat\chat_history.db"
    ) -> AsyncGenerator["ChatBot", None]:
        """
        å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚æ–¹æ³•ï¼šåˆ›å»ºä½¿ç”¨ AsyncSqliteSaver æŒä¹…åŒ–çš„ ChatBot å®ä¾‹ã€‚
        
        ä½¿ç”¨æ–¹å¼:
            async with ChatBot.create_with_persistence() as chatbot:
                async for content in chatbot.chat("ä½ å¥½"):
                    print(content)
        
        Args:
            db_path: SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            
        Yields:
            ä½¿ç”¨ AsyncSqliteSaver çš„ ChatBot å®ä¾‹
        """
        async with AsyncSqliteSaver.from_conn_string(str(db_path)) as checkpointer:
            yield cls(checkpointer)
    
    async def intent_router(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        æ„å›¾è¯†åˆ«
        """
        promot = intent_router_template.format(
            question=main_state.messages[-1].content,
        )
        llm = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=False,temperature=0.5)
        result = await llm.ainvoke(promot) 
        self.update_usage(result)
        
        # å»æ‰ LLM è¿”å›å†…å®¹ä¸­çš„å¼•å·ï¼ˆLLM æœ‰æ—¶ä¼šè¿”å›å¸¦å¼•å·çš„å­—ç¬¦ä¸²ï¼‰
        intent_content = result.content.strip().strip('"').strip("'")
        
        # æ£€æŸ¥resultæ˜¯å¦åœ¨feature_listä¸­
        if intent_content not in self.feature_list:
            raise LLMParseError(
                message=f"æ— æ•ˆçš„åŠŸèƒ½åˆ†ç±»: '{intent_content}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=result.content  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        logger.debug(f"æ„å›¾è¯†åˆ«ç»“æœ: {intent_content}")
        return {
            "intent" : [intent_content]
        } 
    
    async def feat_intro_router(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        åŠŸèƒ½ä»‹ç»è·¯ç”±
        """
        from lifewatch.llm.llm_classify.utils.user_guide_parser import load_user_guide
        from lifewatch.llm.llm_classify.schemas.user_guide_schemas import SummaryOption
        llm = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=False,temperature=0.5)
        guide = load_user_guide()
        all_ids = guide.get_all_ids()
        # ç¬¬ä¸€æ¬¡è·¯ç”±
        option = SummaryOption(id = True,title = False,abstract = True)
        outline = guide.transform_to_table(guide.get_children_summary(options=option))
        result = await llm.ainvoke(intro_router_template.format(
            question=main_state.messages[-1].content,
            outline=outline,
        ))
        self.update_usage(result)
        # åˆ¤æ–­id_listæ˜¯å¦åŒ…å«åœ¨idä¸­
        id_list = json.loads(result.content)
        logger.debug(f"è·¯ç”±ç»“æœ: {id_list}")
        
        # è·å–æ–°çš„outline
        outline = []
        for id in id_list:
            if id in all_ids:
                outline += guide.get_children_summary(id, options=option)
        if outline == []:
            logger.error(f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­")
            raise LLMParseError(
                message=f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=id_list  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šç»†ç­›
        logger.debug("\n=== ç¬¬2æ­¥ï¼šç»†ç­›è·¯ç”± ===")
        outline = guide.transform_to_table(outline)
        logger.debug(f"ç»†ç­›èŒƒå›´:\n{outline}")
        result = await llm.ainvoke(intro_router_template.format(outline=outline, question=main_state.messages[-1].content))
        self.update_usage(result)
        id_list = json.loads(result.content)
        logger.debug(f"è·¯ç”±ç»“æœ: {id_list}")

        # è·å–content
        logger.debug("\n=== ç¬¬3æ­¥ï¼šè·å–å†…å®¹ ===")
        content = ""
        for id in id_list:
            if id in all_ids:
                content += guide.get_section_as_markdown(id,start_level=3,max_heading_depth=3)
                content += "\n"
        if content == "":
            logger.error(f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­")
            raise LLMParseError(
                message=f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=id_list  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        logger.debug(f"è·å–çš„å†…å®¹:\n{content}")

        
        self.update_usage(result)
        logger.debug(f"åŠŸèƒ½ä»‹ç»ç»“æœ:\n{result.content}")
        # æ‰“å° usage ç»Ÿè®¡
        logger.debug("\n=== Token Usage ç»Ÿè®¡ ===")
        # logger.debug(f"è°ƒç”¨æ¬¡æ•°: {self.tokens_usage[self.thread_id]['call_count']}")
        logger.debug(f"è¾“å…¥ Tokens: {self.tokens_usage[self.thread_id]['input_tokens']}")
        logger.debug(f"è¾“å‡º Tokens: {self.tokens_usage[self.thread_id]['output_tokens']}")
        logger.debug(f"æ€» Tokens: {self.tokens_usage[self.thread_id]['total_tokens']}")


        return {
            "guide_content" : [content]
        } 
    
    async def feature_introduce(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        åŠŸèƒ½ä»‹ç»
        """
        # è®¾ç½®å†å²æ¶ˆæ¯
        history_messages = get_history_messages(main_state.messages)
        prompt = intro_template.format(
            question=main_state.messages[-1].content,
            guide_content=main_state.guide_content[-1],
            history_messages=history_messages
        )
        result = await self.llm_streaming.ainvoke(prompt)
        self.update_usage(result)
        return {
            "messages" : [result]
        }
    
    async def norm_chat(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        history_messages = get_history_messages(main_state.messages)
        prompt = norm_chat_template.format(
            question=main_state.messages[-1].content,
            history_messages=history_messages
        )
        result = await self.llm_streaming.ainvoke(prompt)
        self.update_usage(result)
        return {
            "messages" : [result]
        }
    
    async def chat_not_stream(self, user_input: str, thread_id: str = None) -> str:
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒå¯¹è¯ã€‚å¦‚æœä¸ä¼ åˆ™ä½¿ç”¨ self.thread_id
            
        Returns:
            AI çš„å›å¤å†…å®¹
        """
        from langchain_core.messages import HumanMessage
        
        # ä½¿ç”¨ä¼ å…¥çš„ thread_id æˆ–è€…å·²è®¾ç½®çš„ thread_id
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        # åªæœ‰ä¼ å…¥ thread_id æ—¶æ‰æ›´æ–°
        if thread_id is not None:
            self.set_thread_id(thread_id)
        # è°ƒç”¨ç¼–è¯‘åçš„ graph
        result = await self.chatbot.ainvoke(
            {"messages": [HumanMessage(content=user_input)]},
            config  = self.config
        )
        
        # è¿”å›æœ€åä¸€æ¡ AI æ¶ˆæ¯çš„å†…å®¹
        return result["messages"][-1].content
    
    async def chat_stream(self, user_input: str, thread_id: str = None):
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯ID
            
        Yields:
            AI å›å¤çš„å†…å®¹ç‰‡æ®µ
        """
        from langchain_core.messages import HumanMessage, AIMessageChunk
        
        # ä½¿ç”¨ä¼ å…¥çš„ thread_id æˆ–è€…å·²è®¾ç½®çš„ thread_id
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        if thread_id is not None:
            self.set_thread_id(thread_id)
        
        # ä½¿ç”¨ astream è¿›è¡Œæµå¼è¾“å‡º
        # stream_mode="messages" ä¼šæµå¼è¾“å‡ºæ‰€æœ‰æ¶ˆæ¯äº‹ä»¶
        async for event in self.chatbot.astream(
            {"messages": [HumanMessage(content=user_input)]},
            config=self.config,
            stream_mode="messages"
        ):
            # event æ˜¯ä¸€ä¸ª tuple: (message, metadata)
            if len(event) >= 1:
                message = event[0]
                # åªè¾“å‡º AI æ¶ˆæ¯çš„å†…å®¹
                if isinstance(message, AIMessageChunk) and message.content:
                    yield message.content
    
    async def chat_stream_with_status(self, user_input: str, thread_id: str = None):
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å›å¤ï¼ˆå¸¦çŠ¶æ€ä¿¡æ¯ï¼‰
        
        å‰ç«¯å¯ä»¥æ ¹æ® type åŒºåˆ†ï¼š
        - type="status": å½“å‰æ‰§è¡Œçš„æ­¥éª¤ï¼ˆèŠ‚ç‚¹å¼€å§‹æ—¶è§¦å‘ï¼‰
        - type="content": AI å›å¤çš„å†…å®¹ç‰‡æ®µ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯ID
            
        Yields:
            dict: {"type": "status"|"content", "message": str, "node": str}
        """
        
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        if thread_id is not None:
            self.set_thread_id(thread_id)
        
        # èŠ‚ç‚¹åç§°åˆ°ä¸­æ–‡æè¿°çš„æ˜ å°„
        node_names = {
            "intent_router": "æ­£åœ¨è¯†åˆ«æ„å›¾...",
            "feat_intro_router": "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...",
            "feature_introduce": "æ­£åœ¨ç”Ÿæˆå›ç­”...",
            "norm_chat": "æ­£åœ¨ç”Ÿæˆå›ç­”...",
        }
        
        last_node = None  # è®°å½•ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œé¿å…é‡å¤å‘é€çŠ¶æ€
        
        # ä½¿ç”¨ astream_events è·å–æ›´è¯¦ç»†çš„äº‹ä»¶ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹å¼€å§‹ï¼‰
        async for event in self.chatbot.astream_events(
            {"messages": [HumanMessage(content=user_input)]},
            config=self.config,
            version="v2"  # ä½¿ç”¨ v2 ç‰ˆæœ¬çš„äº‹ä»¶æ ¼å¼
        ):
            event_type = event.get("event", "")
            
            # èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
            if event_type == "on_chain_start":
                node_name = event.get("name", "")
                if node_name in node_names and node_name != last_node:
                    last_node = node_name
                    yield {
                        "type": "status",
                        "node": node_name,
                        "message": node_names[node_name]
                    }
            
            # æ¶ˆæ¯æµå¼è¾“å‡ºäº‹ä»¶
            elif event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content") and chunk.content:
                    yield {
                        "type": "content",
                        "node": last_node,
                        "message": chunk.content
                    }

async def main():
    # ä½¿ç”¨æŒä¹…åŒ–ä¿å­˜å™¨ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    async with ChatBot.create_with_persistence() as app:
        app.set_thread_id("test_stream_status")
        while True:
            user_input = input("User: ")
            if user_input == "exit":
                break
            
            print()  # æ¢è¡Œ
            async for event in app.chat_stream_with_status(user_input):
                if event["type"] == "status":
                    # æ˜¾ç¤ºå½“å‰æ­¥éª¤
                    print(f"ğŸ”„ {event['message']}")
                elif event["type"] == "content":
                    # æ˜¾ç¤º AI å›å¤å†…å®¹
                    print(event["message"], end="", flush=True)
            print()  # æ¢è¡Œ

from asyncio import run
if __name__ == "__main__":
    run(main())