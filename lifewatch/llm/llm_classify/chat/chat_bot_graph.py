"""
V2 ChatBot æ”¹ä¸ºä½¿ç”¨graph å¢åŠ åŠŸèƒ½è§£è¯´å’Œç›¸å…³åŠŸèƒ½è§£ç­”
"""
from lifewatch.llm.llm_classify.schemas.chatbot_schemas import ChatBotSchemas
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.checkpoint.memory import InMemorySaver
from typing import Optional, Union, AsyncGenerator, Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager
from lifewatch.llm.custom_prompt.common_prompt import intent_router_template,norm_chat_template
from lifewatch.llm.custom_prompt.chatbot_prompt.feature_introduce import intro_template,intro_router_template
from lifewatch.llm.llm_classify.utils import create_ChatTongyiModel
import json
import traceback
from datetime import datetime
from lifewatch.utils import get_logger
import logging
from langchain_core.messages import HumanMessage, AIMessage,AIMessageChunk,ToolMessage
from langgraph.graph import StateGraph
from langgraph.types import RetryPolicy
from lifewatch.llm.llm_classify.tools.database_tools import get_user_behavior_stats
logger = get_logger(__name__,logging.DEBUG)
class LLMParseError(Exception):
    """
    LLM è¾“å‡ºè§£æé”™è¯¯ - å¯é‡è¯•
    
    å½“ LLM è¿”å›çš„å†…å®¹æ— æ³•æ­£ç¡®è§£æï¼ˆå¦‚ JSON æ ¼å¼é”™è¯¯ã€ç¼ºå°‘å¿…è¦å­—æ®µç­‰ï¼‰æ—¶æŠ›å‡ºæ­¤é”™è¯¯ã€‚
    æ­¤é”™è¯¯ç±»å‹è¢«æ ‡è®°ä¸ºå¯é‡è¯•ï¼Œé‡è¯•æœºåˆ¶ä¼šæ•è·æ­¤é”™è¯¯å¹¶é‡æ–°è°ƒç”¨ LLMã€‚
    
    Attributes:
        message: é”™è¯¯æè¿°ä¿¡æ¯
        original_error: åŸå§‹å¼‚å¸¸ï¼ˆå¯é€‰ï¼‰
        raw_content: LLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
    """
    def __init__(self, message: str, original_error: Exception = None, raw_content: str = None):
        super().__init__(message)
        self.message = message
        self.original_error = original_error
        self.raw_content = raw_content

    def __str__(self):
        base_msg = f"LLMParseError: {self.message}"
        if self.raw_content:
            # æˆªæ–­è¿‡é•¿çš„åŸå§‹å†…å®¹
            content_preview = self.raw_content[:100] + "..." if len(self.raw_content) > 100 else self.raw_content
            base_msg += f"\nåŸå§‹å†…å®¹: {content_preview}"
        return base_msg


def get_history_messages(messages: list[HumanMessage| AIMessage]):
    history_messages = ""
    for i,msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            history_messages += f"{i}. User: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_messages += f"{i}. Assistant: {msg.content}\n"
    return history_messages

class ChatBot:
    def __init__(self,checkpointer: Optional[Union[InMemorySaver, AsyncSqliteSaver]] = None):
        self.current_total_tokens = 0
        # tokens_usage: æ¯è½®å¯¹è¯çš„ä½¿ç”¨é‡ï¼ˆæ¯è½®å¯¹è¯å‰æ¸…ç©ºï¼‰
        self.tokens_usage: Dict[str, Dict[str, int]] = {}
        # session_tokens_usage: ä¼šè¯ç´¯è®¡ä½¿ç”¨é‡ï¼ˆæŒç»­ç´¯åŠ ï¼‰
        self.session_tokens_usage: Dict[str, Dict[str, int]] = {}
        self.checkpointer = checkpointer or InMemorySaver()
        # ç”¨äºæµå¼è¾“å‡º
        self.llm_streaming = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=True,temperature=0.5)
        self.config: Optional[dict] = None
        self.thread_id = None
        # self._is_persistent = isinstance(self.checkpointer, AsyncSqliteSaver)
        # è¿™é‡Œçš„feature_listå¿…é¡»ä¸lifewatch\llm\custom_prompt\common_prompt.py
        # ä¸­çš„intent_router_templateä¸­çš„feature_listä¿æŒä¸€è‡´
        self.feature_list = ["lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£","ä¸€èˆ¬æ¨¡å¼"] # 
        self.graph = StateGraph(ChatBotSchemas)
        self.chatbot = self._build_graph()
    def _build_graph(self):
        """
        æ„å»ºå¯¹è¯æµç¨‹å›¾
        
        æµç¨‹ï¼š
        START â†’ intent_router â†’ (æ ¹æ®æ„å›¾åˆ†æ”¯)
            - "lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£" â†’ feat_intro_router â†’ feature_introduce â†’ END
            - å…¶ä»–æ„å›¾ â†’ norm_chat â†’ (æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨?)
                - æœ‰ â†’ tool_node â†’ tool_result_handler â†’ END
                - æ—  â†’ END
        """
        from langgraph.graph import START, END
        
        # æ·»åŠ èŠ‚ç‚¹
        self.graph.add_node("intent_router",
                            self.intent_router,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("feat_intro_router",
                            self.feat_intro_router,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("feature_introduce",
                            self.feature_introduce,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError],max_attempts=2))
        self.graph.add_node("norm_chat",
                            self.norm_chat,
                            retry_policy=RetryPolicy(retry_on=[LLMParseError], max_attempts=2))
        self.graph.add_node("tool_node", self.tool_node)
        self.graph.add_node("tool_result_handler", self.tool_result_handler)
        
        # å®šä¹‰æ¡ä»¶è·¯ç”±å‡½æ•°
        def route_by_intent(main_state: ChatBotSchemas) -> str:
            """æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹"""
            intent = main_state["intent"][-1] if main_state["intent"] else ""
            if intent == "lifeprismè½¯ä»¶ä½¿ç”¨å’Œè®²è§£":
                return "feat_intro_router"
            else:
                return "norm_chat"
        
        def route_after_norm_chat(main_state: ChatBotSchemas) -> str:
            """åˆ¤æ–­ norm_chat åæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·"""
            last_message = main_state["messages"][-1] if main_state["messages"] else None
            if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                return "tool_node"
            return END
        
        # æ·»åŠ è¾¹
        # START â†’ intent_router
        self.graph.add_edge(START, "intent_router")
        
        # intent_router â†’ æ¡ä»¶åˆ†æ”¯
        self.graph.add_conditional_edges(
            "intent_router",
            route_by_intent,
            {
                "feat_intro_router": "feat_intro_router",
                "norm_chat": "norm_chat"
            }
        )
        
        # feat_intro_router â†’ feature_introduce
        self.graph.add_edge("feat_intro_router", "feature_introduce")
        
        # feature_introduce â†’ END
        self.graph.add_edge("feature_introduce", END)
        
        # norm_chat â†’ æ¡ä»¶åˆ†æ”¯ï¼ˆåˆ¤æ–­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼‰
        self.graph.add_conditional_edges(
            "norm_chat",
            route_after_norm_chat,
            {
                "tool_node": "tool_node",
                END: END
            }
        )
        
        # tool_node â†’ tool_result_handler
        self.graph.add_edge("tool_node", "tool_result_handler")
        
        # tool_result_handler â†’ END
        self.graph.add_edge("tool_result_handler", END)
        
        # ç¼–è¯‘ graphï¼Œä¼ å…¥ checkpointer
        return self.graph.compile(checkpointer=self.checkpointer)


    def init_tokens_usage(self, thread_id: str):
        """
        åˆå§‹åŒ–ä¼šè¯çš„ token ä½¿ç”¨æƒ…å†µï¼ˆä»…åœ¨ä¸å­˜åœ¨æ—¶åˆå§‹åŒ–ï¼‰
        
        - tokens_usage: æ¯è½®å¯¹è¯çš„ä½¿ç”¨é‡
        - session_tokens_usage: ä¼šè¯ç´¯è®¡ä½¿ç”¨é‡
        """
        logger.debug(f"åˆå§‹åŒ–tokenä½¿ç”¨æƒ…å†µ: {thread_id}")
        
        # ä»…åœ¨ä¸å­˜åœ¨æ—¶åˆå§‹åŒ–
        if thread_id not in self.tokens_usage:
            self.tokens_usage[thread_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "search_count": 0
            }
        
        if thread_id not in self.session_tokens_usage:
            self.session_tokens_usage[thread_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "search_count": 0
            }
    
    def reset_turn_usage(self):
        """
        æ¸…ç©ºæœ¬è½®å¯¹è¯çš„ tokens_usageï¼ˆæ¯æ¬¡ç”¨æˆ·å‘é€æ¶ˆæ¯æ—¶è°ƒç”¨ï¼‰
        """
        if self.thread_id and self.thread_id in self.tokens_usage:
            self.tokens_usage[self.thread_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "search_count": 0
            }
            logger.debug(f"æ¸…ç©ºæœ¬è½®å¯¹è¯ä½¿ç”¨é‡: {self.thread_id}")

    def set_thread_id(self, thread_id: str):
        """
        è®¾ç½®å½“å‰ä¼šè¯çš„ thread_idã€‚
        
        Args:
            thread_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯
        """
        logger.debug(f"è®¾ç½®thread_id: {thread_id}")
        self.config = {"configurable": {"thread_id": thread_id}}
        self.thread_id = thread_id
        self.init_tokens_usage(thread_id)
    def get_new_agent(self,enable_search:bool,enable_thinking:bool,enable_streaming:bool,temperature:float):
        """
        ç”¨äºè·å–æ–°çš„agent
        """
        logger.debug(f"è·å–æ–°çš„agent: enable_search={enable_search}, enable_thinking={enable_thinking}, enable_streaming={enable_streaming}, temperature={temperature}")
        return create_ChatTongyiModel(enable_search=enable_search,
                            enable_thinking=enable_thinking,
                            enable_streaming=enable_streaming,temperature=temperature)
    def update_usage(self, result):
        """
        æ›´æ–° token ä½¿ç”¨é‡
        
        åŒæ—¶æ›´æ–°:
        - tokens_usage: æœ¬è½®å¯¹è¯ä½¿ç”¨é‡
        - session_tokens_usage: ä¼šè¯ç´¯è®¡ä½¿ç”¨é‡
        """
        logger.debug(f"[update_usage] result type: {type(result)}")
        logger.debug(f"[update_usage] result: {result}")
        
        # æ£€æŸ¥ result æ˜¯å¦æœ‰ response_metadata å±æ€§
        if not hasattr(result, 'response_metadata'):
            logger.error(f"[update_usage] result æ²¡æœ‰ response_metadata å±æ€§! result type: {type(result)}")
            logger.error(f"[update_usage] result å†…å®¹: {result}")
            return
        
        logger.debug(f"[update_usage] response_metadata: {result.response_metadata}")
        
        token_usage = result.response_metadata.get("token_usage", {})
        input_tokens = token_usage.get("input_tokens", 0)
        output_tokens = token_usage.get("output_tokens", 0)
        total_tokens = token_usage.get("total_tokens", 0)
        
        # æ›´æ–°æœ¬è½®å¯¹è¯ä½¿ç”¨é‡
        self.tokens_usage[self.thread_id]["input_tokens"] += input_tokens
        self.tokens_usage[self.thread_id]["output_tokens"] += output_tokens
        self.tokens_usage[self.thread_id]["total_tokens"] += total_tokens
        
        # æ›´æ–°ä¼šè¯ç´¯è®¡ä½¿ç”¨é‡
        self.session_tokens_usage[self.thread_id]["input_tokens"] += input_tokens
        self.session_tokens_usage[self.thread_id]["output_tokens"] += output_tokens
        self.session_tokens_usage[self.thread_id]["total_tokens"] += total_tokens

        

    @classmethod
    @asynccontextmanager
    async def create_with_persistence(
        cls,
        db_path: Union[str, Path] = r"lifewatch\llm\llm_classify\chat\chat_history.db"
    ) -> AsyncGenerator["ChatBot", None]:
        """
        å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚æ–¹æ³•ï¼šåˆ›å»ºä½¿ç”¨ AsyncSqliteSaver æŒä¹…åŒ–çš„ ChatBot å®ä¾‹ã€‚
        
        ä½¿ç”¨æ–¹å¼:
            async with ChatBot.create_with_persistence() as chatbot:
                async for content in chatbot.chat("ä½ å¥½"):
                    print(content)
        
        Args:
            db_path: SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            
        Yields:
            ä½¿ç”¨ AsyncSqliteSaver çš„ ChatBot å®ä¾‹
        """
        async with AsyncSqliteSaver.from_conn_string(str(db_path)) as checkpointer:
            yield cls(checkpointer)
    

        
    # ===============================================================
    # nodes 
    # ===============================================================
    async def intent_router(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        æ„å›¾è¯†åˆ«
        """
        promot = intent_router_template.format(
            question=main_state["current_human_message"],
        )
        llm = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=False,temperature=0.5)
        logger.debug(f"[intent_router] è°ƒç”¨ LLM...")
        try:
            result = await llm.ainvoke(promot)
            logger.debug(f"[intent_router] LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[intent_router] LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[intent_router] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        
        # å»æ‰ LLM è¿”å›å†…å®¹ä¸­çš„å¼•å·ï¼ˆLLM æœ‰æ—¶ä¼šè¿”å›å¸¦å¼•å·çš„å­—ç¬¦ä¸²ï¼‰
        intent_content = result.content.strip().strip('"').strip("'")
        
        # æ£€æŸ¥resultæ˜¯å¦åœ¨feature_listä¸­
        if intent_content not in self.feature_list:
            raise LLMParseError(
                message=f"æ— æ•ˆçš„åŠŸèƒ½åˆ†ç±»: '{intent_content}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=result.content  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        logger.debug(f"æ„å›¾è¯†åˆ«ç»“æœ: {intent_content}")
        return {
            "intent" : [intent_content]
        } 
    
    async def feat_intro_router(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        åŠŸèƒ½ä»‹ç»è·¯ç”±
        """
        from lifewatch.llm.llm_classify.utils.user_guide_parser import load_user_guide
        from lifewatch.llm.llm_classify.schemas.user_guide_schemas import SummaryOption
        llm = self.get_new_agent(enable_search=False,
                            enable_thinking=False,
                            enable_streaming=False,temperature=0.5)
        guide = load_user_guide()
        all_ids = guide.get_all_ids()
        # ç¬¬ä¸€æ¬¡è·¯ç”±
        option = SummaryOption(id = True,title = False,abstract = True)
        outline = guide.transform_to_table(guide.get_children_summary(options=option))
        logger.debug(f"[feat_intro_router] ç¬¬ä¸€æ¬¡è·¯ç”±è°ƒç”¨ LLM...")
        try:
            result = await llm.ainvoke(intro_router_template.format(
                question=main_state["current_human_message"],
                outline=outline,
            ))
            logger.debug(f"[feat_intro_router] LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[feat_intro_router] ç¬¬ä¸€æ¬¡è·¯ç”± LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[feat_intro_router] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        # åˆ¤æ–­id_listæ˜¯å¦åŒ…å«åœ¨idä¸­
        id_list = json.loads(result.content)
        logger.debug(f"è·¯ç”±ç»“æœ: {id_list}")
        
        # è·å–æ–°çš„outline
        outline = []
        for id in id_list:
            if id in all_ids:
                outline += guide.get_children_summary(id, options=option)
        if outline == []:
            logger.error(f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­")
            raise LLMParseError(
                message=f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=id_list  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šç»†ç­›
        logger.debug("\n=== ç¬¬2æ­¥ï¼šç»†ç­›è·¯ç”± ===")
        outline = guide.transform_to_table(outline)
        logger.debug(f"ç»†ç­›èŒƒå›´:\n{outline}")
        logger.debug(f"[feat_intro_router] ç¬¬äºŒæ¬¡è·¯ç”±è°ƒç”¨ LLM...")
        try:
            result = await llm.ainvoke(intro_router_template.format(outline=outline, question=main_state["current_human_message"]))
            logger.debug(f"[feat_intro_router] ç¬¬äºŒæ¬¡è·¯ç”± LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[feat_intro_router] ç¬¬äºŒæ¬¡è·¯ç”± LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[feat_intro_router] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        id_list = json.loads(result.content)
        logger.debug(f"è·¯ç”±ç»“æœ: {id_list}")

        # è·å–content
        logger.debug("\n=== ç¬¬3æ­¥ï¼šè·å–å†…å®¹ ===")
        content = ""
        for id in id_list:
            if id in all_ids:
                content += guide.get_section_as_markdown(id,start_level=3,max_heading_depth=3)
                content += "\n"
        if content == "":
            logger.error(f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­")
            raise LLMParseError(
                message=f"æ— æ•ˆçš„idåˆ—è¡¨: '{id_list}' ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­",
                raw_content=id_list  # ä¿å­˜åŸå§‹è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
            )
        logger.debug(f"è·å–çš„å†…å®¹:\n{content}")

        
        self.update_usage(result)
        logger.debug(f"åŠŸèƒ½ä»‹ç»ç»“æœ:\n{result.content}")
        # æ‰“å° usage ç»Ÿè®¡
        logger.debug("\n=== Token Usage ç»Ÿè®¡ ===")
        # logger.debug(f"è°ƒç”¨æ¬¡æ•°: {self.tokens_usage[self.thread_id]['call_count']}")
        logger.debug(f"è¾“å…¥ Tokens: {self.tokens_usage[self.thread_id]['input_tokens']}")
        logger.debug(f"è¾“å‡º Tokens: {self.tokens_usage[self.thread_id]['output_tokens']}")
        logger.debug(f"æ€» Tokens: {self.tokens_usage[self.thread_id]['total_tokens']}")


        return {
            "guide_content" : [content]
        } 
    
    async def feature_introduce(self,main_state:ChatBotSchemas)->ChatBotSchemas:
        """
        åŠŸèƒ½ä»‹ç»
        """
        # è®¾ç½®å†å²æ¶ˆæ¯
        history_messages = get_history_messages(main_state["messages"])
        prompt = intro_template.format(
            question=main_state["current_human_message"],
            guide_content=main_state["guide_content"][-1],
            history_messages=history_messages
        )
        logger.debug(f"[feature_introduce] è°ƒç”¨ LLM...")
        try:
            result = await self.llm_streaming.ainvoke(prompt)
            logger.debug(f"[feature_introduce] LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[feature_introduce] LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[feature_introduce] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        return {
            "messages" : [result]
        }
    
    # å¯ç”¨å·¥å…·é›†åˆï¼ˆç”¨äºéªŒè¯ LLM è¿”å›çš„å·¥å…·è°ƒç”¨ï¼‰
    VALID_TOOLS = {"get_user_behavior_stats"}
    
    async def norm_chat(self, main_state: ChatBotSchemas) -> ChatBotSchemas:
        # å½“å‰çš„æ—¶é—´
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        history_messages = get_history_messages(main_state["messages"])
        prompt = norm_chat_template.format(
            question=main_state["current_human_message"],
            history_messages=history_messages,
            custom_prompt=f"å½“å‰æ—¶é—´: {current_time}"
        )
        llm_with_tool = self.llm_streaming.bind_tools([get_user_behavior_stats])
        logger.debug(f"[norm_chat] è°ƒç”¨ LLM (with tools)...")
        try:
            result = await llm_with_tool.ainvoke(prompt)
            logger.debug(f"[norm_chat] LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[norm_chat] LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[norm_chat] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        
        # éªŒè¯å·¥å…·è°ƒç”¨æ˜¯å¦æœ‰æ•ˆ
        if hasattr(result, 'tool_calls') and result.tool_calls:
            for tool_call in result.tool_calls:
                tool_name = tool_call.get("name", "")
                if tool_name not in self.VALID_TOOLS:
                    logger.warning(f"LLM è¯·æ±‚äº†æœªçŸ¥å·¥å…·: {tool_name}")
                    raise LLMParseError(
                        message=f"LLM è¯·æ±‚äº†æœªçŸ¥å·¥å…·: {tool_name}ï¼Œå¯ç”¨å·¥å…·: {self.VALID_TOOLS}",
                        raw_content=str(result.tool_calls)
                    )
        
        return {
            "messages": [result]
        }
    
    async def tool_node(self, main_state: ChatBotSchemas) -> ChatBotSchemas:
        """
        å¤„ç†å·¥å…·è°ƒç”¨çš„èŠ‚ç‚¹ï¼Œæ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
        """ 
        # 1. è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨è¯·æ±‚
        last_message = main_state["messages"][-1]
        if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
            logger.warning("tool_node è¢«è°ƒç”¨ä½†æ²¡æœ‰ tool_calls")
            return {}
        
        # 2. å·¥å…·æ˜ å°„è¡¨
        tool_map = {
            "get_user_behavior_stats": get_user_behavior_stats
        }
        
        # 3. æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_messages = []
        tool_results = []
        
        for tool_call in last_message.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_call_id = tool_call["id"]
            
            logger.debug(f"æ‰§è¡Œå·¥å…·è°ƒç”¨: {tool_name}, å‚æ•°: {tool_args}")
            
            if tool_name in tool_map:
                try:
                    # æ‰§è¡Œå·¥å…·ï¼ˆä½¿ç”¨ invoke æ–¹æ³•ï¼‰
                    tool_result = tool_map[tool_name].invoke(tool_args)
                    result_str = json.dumps(tool_result, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
                    result_str = json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
            else:
                logger.error(f"æœªçŸ¥å·¥å…·: {tool_name}")
                result_str = json.dumps({"success": False, "error": f"æœªçŸ¥å·¥å…·: {tool_name}"}, ensure_ascii=False)
            
            # åˆ›å»º ToolMessage
            tool_messages.append(ToolMessage(
                content=result_str,
                tool_call_id=tool_call_id
            ))
            tool_results.append(result_str)
        
        logger.debug(f"å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æœæ•°é‡: {len(tool_results)}")
        
        # 4. è¿”å›ç»“æœ
        return {
            "messages": tool_messages,
            "tools_result": tool_results
        }
    
    async def tool_result_handler(self, main_state: ChatBotSchemas) -> ChatBotSchemas:
        """
        ç»“åˆå·¥å…·è°ƒç”¨ç»“æœä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ï¼ˆä¸ç»‘å®šå·¥å…·ï¼ŒèŠ‚çœ tokensï¼‰
        """
        from lifewatch.llm.custom_prompt.common_prompt import tool_result_template
        
        # 1. è·å–å†å²å¯¹è¯ï¼ˆget_history_messages åªå¤„ç† HumanMessage å’Œ AIMessageï¼Œè‡ªåŠ¨å¿½ç•¥ ToolMessageï¼‰
        history_messages = get_history_messages(main_state["messages"])
        
        # 2. è·å–å·¥å…·è¿”å›ç»“æœ
        tool_result = "\n".join(main_state["tools_result"]) if main_state["tools_result"] else ""
        
        # 3. æ„å»º prompt
        prompt = tool_result_template.format(
            history_messages=history_messages,
            question=main_state["current_human_message"],
            tool_result=tool_result
        )
        
        logger.debug(f"tool_result_handler prompt æ„å»ºå®Œæˆ")
        
        # 4. è°ƒç”¨ LLMï¼ˆä¸ç»‘å®šå·¥å…·ï¼‰
        logger.debug(f"[tool_result_handler] è°ƒç”¨ LLM...")
        try:
            result = await self.llm_streaming.ainvoke(prompt)
            logger.debug(f"[tool_result_handler] LLM è¿”å› result type: {type(result)}")
        except Exception as e:
            logger.error(f"[tool_result_handler] LLM è°ƒç”¨å¤±è´¥: {e}")
            logger.error(f"[tool_result_handler] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            raise
        self.update_usage(result)
        
        return {
            "messages": [result]
        }

        

    
    # ===============================================================
    # chat æ¥å£ not streamï¼›stream ; stream_with_status
    # ===============================================================
    async def chat_not_stream(self, user_input: str, thread_id: str = None) -> str:
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒå¯¹è¯ã€‚å¦‚æœä¸ä¼ åˆ™ä½¿ç”¨ self.thread_id
            
        Returns:
            AI çš„å›å¤å†…å®¹
        """
        from langchain_core.messages import HumanMessage
        
        # ä½¿ç”¨ä¼ å…¥çš„ thread_id æˆ–è€…å·²è®¾ç½®çš„ thread_id
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        # åªæœ‰ä¼ å…¥ thread_id æ—¶æ‰æ›´æ–°
        if thread_id is not None:
            self.set_thread_id(thread_id)
        # è°ƒç”¨ç¼–è¯‘åçš„ graph
        result = await self.chatbot.ainvoke(
            {
                "messages": [HumanMessage(content=user_input)],
                "current_human_message": user_input,
                "intent": [],
                "guide_content": [],
                "tools_result": []
            },
            config=self.config
        )
        
        # è¿”å›æœ€åä¸€æ¡ AI æ¶ˆæ¯çš„å†…å®¹
        return result["messages"][-1].content
    
    async def chat_stream(self, user_input: str, thread_id: str = None):
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯ID
            
        Yields:
            AI å›å¤çš„å†…å®¹ç‰‡æ®µ
        """
        from langchain_core.messages import HumanMessage, AIMessageChunk
        
        # ä½¿ç”¨ä¼ å…¥çš„ thread_id æˆ–è€…å·²è®¾ç½®çš„ thread_id
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        if thread_id is not None:
            self.set_thread_id(thread_id)
        
        # æ¸…ç©ºæœ¬è½®å¯¹è¯ä½¿ç”¨é‡
        self.reset_turn_usage()
        
        # ä½¿ç”¨ astream è¿›è¡Œæµå¼è¾“å‡º
        # stream_mode="messages" ä¼šæµå¼è¾“å‡ºæ‰€æœ‰æ¶ˆæ¯äº‹ä»¶
        async for event in self.chatbot.astream(
            {
                "messages": [HumanMessage(content=user_input)],
                "current_human_message": user_input,
                "intent": [],
                "guide_content": [],
                "tools_result": []
            },
            config=self.config,
            stream_mode="messages"
        ):
            # event æ˜¯ä¸€ä¸ª tuple: (message, metadata)
            if len(event) >= 1:
                message = event[0]
                # åªè¾“å‡º AI æ¶ˆæ¯çš„å†…å®¹
                if isinstance(message, AIMessageChunk) and message.content:
                    yield message.content
    
    async def chat_stream_with_status(self, user_input: str, thread_id: str = None):
        """
        å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å›å¤ï¼ˆå¸¦çŠ¶æ€ä¿¡æ¯ï¼‰
        
        å‰ç«¯å¯ä»¥æ ¹æ® type åŒºåˆ†ï¼š
        - type="status": å½“å‰æ‰§è¡Œçš„æ­¥éª¤ï¼ˆèŠ‚ç‚¹å¼€å§‹æ—¶è§¦å‘ï¼‰
        - type="content": AI å›å¤çš„å†…å®¹ç‰‡æ®µ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            thread_id: ä¼šè¯ID
            
        Yields:
            dict: {"type": "status"|"content", "message": str, "node": str}
        """
        
        if thread_id is None and self.thread_id is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_thread_id() æˆ–ä¼ å…¥ thread_id å‚æ•°")
        
        if thread_id is not None:
            self.set_thread_id(thread_id)
        
        # æ¸…ç©ºæœ¬è½®å¯¹è¯ä½¿ç”¨é‡
        self.reset_turn_usage()
        
        # èŠ‚ç‚¹åç§°åˆ°ä¸­æ–‡æè¿°çš„æ˜ å°„
        node_names = {
            "intent_router": "æ­£åœ¨è¯†åˆ«æ„å›¾...",
            "feat_intro_router": "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...",
            "feature_introduce": "æ­£åœ¨ç”Ÿæˆå›ç­”...",
            "norm_chat": "æ­£åœ¨ç”Ÿæˆå›ç­”...",
            "tool_node": "æ­£åœ¨æŸ¥è¯¢æ•°æ®...",
            "tool_result_handler": "æ­£åœ¨æ•´åˆæ•°æ®ç”Ÿæˆå›ç­”...",
        }
        
        last_node = None  # è®°å½•ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œé¿å…é‡å¤å‘é€çŠ¶æ€
        
        logger.debug(f"[chat_stream_with_status] å¼€å§‹ astream_events, thread_id={self.thread_id}")
        
        try:
            # ä½¿ç”¨ astream_events è·å–æ›´è¯¦ç»†çš„äº‹ä»¶ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹å¼€å§‹ï¼‰
            async for event in self.chatbot.astream_events(
                {
                    "messages": [HumanMessage(content=user_input)],
                    "current_human_message": user_input,
                    "intent": [],
                    "guide_content": [],
                    "tools_result": []
                },
                config=self.config,
                version="v2"  # ä½¿ç”¨ v2 ç‰ˆæœ¬çš„äº‹ä»¶æ ¼å¼
            ):
                event_type = event.get("event", "")
                logger.debug(f"[chat_stream_with_status] æ”¶åˆ°äº‹ä»¶: type={event_type}, name={event.get('name', 'N/A')}")
                
                # èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                if event_type == "on_chain_start":
                    node_name = event.get("name", "")
                    if node_name in node_names and node_name != last_node:
                        last_node = node_name
                        logger.debug(f"[chat_stream_with_status] èŠ‚ç‚¹å¼€å§‹: {node_name}")
                        yield {
                            "type": "status",
                            "node": node_name,
                            "message": node_names[node_name]
                        }
                
                # æ¶ˆæ¯æµå¼è¾“å‡ºäº‹ä»¶
                elif event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, "content") and chunk.content:
                        yield {
                            "type": "content",
                            "node": last_node,
                            "message": chunk.content
                        }
                        
        except LLMParseError as e:
            logger.error(f"[chat_stream_with_status] LLM è§£æé‡è¯•å¤±è´¥: {e}")
            logger.error(f"[chat_stream_with_status] å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            yield {
                "type": "error",
                "node": last_node or "unknown",
                "message": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ï¼Œè¯·æ¢ä¸€ç§æ–¹å¼æé—®ã€‚"
            }
        except AttributeError as e:
            # ä¸“é—¨æ•è· AttributeErrorï¼Œå¯èƒ½æ˜¯ 'dict' object has no attribute 'status_code' çš„æ¥æº
            logger.error(f"[chat_stream_with_status] AttributeError: {e}")
            logger.error(f"[chat_stream_with_status] å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            yield {
                "type": "error",
                "node": last_node or "unknown",
                "message": f"å±æ€§é”™è¯¯: {str(e)}"
            }
        except Exception as e:
            logger.error(f"[chat_stream_with_status] æœªçŸ¥é”™è¯¯: {e}")
            logger.error(f"[chat_stream_with_status] é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"[chat_stream_with_status] å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            yield {
                "type": "error",
                "node": last_node or "unknown",
                "message": f"å‘ç”Ÿé”™è¯¯: {str(e)}"
            }

async def main():
    # ä½¿ç”¨æŒä¹…åŒ–ä¿å­˜å™¨ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    async with ChatBot.create_with_persistence() as app:
        app.set_thread_id("test_stream_status")
        while True:
            user_input = input("User: ")
            if user_input == "exit":
                break
            
            print()  # æ¢è¡Œ
            async for event in app.chat_stream_with_status(user_input):
                if event["type"] == "status":
                    # æ˜¾ç¤ºå½“å‰æ­¥éª¤
                    print(f"ğŸ”„ {event['message']}")
                elif event["type"] == "content":
                    # æ˜¾ç¤º AI å›å¤å†…å®¹
                    print(event["message"], end="", flush=True)
            print()  # æ¢è¡Œ

from asyncio import run
if __name__ == "__main__":
    run(main())