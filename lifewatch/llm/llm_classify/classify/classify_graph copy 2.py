"""
LLM åˆ†ç±»å›¾ç®¡ç†å™¨ - ä¼ä¸šçº§å°è£…

ä½¿ç”¨ LangGraph æ„å»ºçš„åˆ†ç±»æµç¨‹ï¼ŒåŒ…å«ï¼š
- å•ç”¨é€”åº”ç”¨åˆ†ç±»
- å¤šç”¨é€”åº”ç”¨åˆ†ç±»ï¼ˆçŸ­æ—¶é•¿/é•¿æ—¶é•¿ï¼‰
- ä½¿ç”¨ InMemoryStore å­˜å‚¨ token ä½¿ç”¨ç»Ÿè®¡
"""

from lifewatch.llm.llm_classify.schemas.classify_shemas import classifyState, Goal, AppInFo, SearchOutput
from langgraph.graph import StateGraph, START, END
from langgraph.store.memory import InMemoryStore
from langchain_core.messages import SystemMessage, HumanMessage
from lifewatch.llm.llm_classify.utils import (
    format_goals_for_prompt,
    format_category_tree_for_prompt,
    format_log_items_table,
    create_ChatTongyiModel,
    split_by_purpose,
    split_by_duartion,
    parse_classification_result,
    extract_json_from_response,
)
import json
import logging
from langgraph.types import Send, RetryPolicy
from typing import Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMClassify:
    """
    LLM åˆ†ç±»å›¾ç®¡ç†å™¨
    
    ä½¿ç”¨ LangGraph æ„å»ºåˆ†ç±»æµç¨‹ï¼Œæ”¯æŒï¼š
    - å•ç”¨é€”åº”ç”¨åˆ†ç±»
    - å¤šç”¨é€”åº”ç”¨åˆ†ç±»ï¼ˆçŸ­æ—¶é•¿/é•¿æ—¶é•¿ï¼‰
    - token ä½¿ç”¨ç»Ÿè®¡ï¼ˆä½¿ç”¨ InMemoryStoreï¼‰
    
    Example:
        classifier = LLMClassify()
        result = classifier.run(state)
        token_summary = classifier.get_token_summary()
    """
    
    # é…ç½®å¸¸é‡
    MAX_LOG_ITEMS = 15
    MAX_TITLE_ITEMS = 5
    SPLIT_DURATION = 10 * 60  # 10min
    
    # Token å­˜å‚¨å‘½åç©ºé—´
    TOKEN_NAMESPACE = ("token_usage",)
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.chat_model = create_ChatTongyiModel()
        self.store = InMemoryStore()
        self._app = None
        self._token_counter = 0  # ç”¨äºç”Ÿæˆå”¯ä¸€ key
        self._build_graph()
    
    def _build_graph(self) -> None:
        """æ„å»º LangGraph å›¾"""
        graph = StateGraph(classifyState)
        
        # æ·»åŠ èŠ‚ç‚¹ï¼ˆå¸¦é‡è¯•ç­–ç•¥çš„èŠ‚ç‚¹ç›´æ¥ä¼ å…¥ retry å‚æ•°ï¼‰
        graph.add_node("get_app_description", self._get_app_description)
        graph.add_node("single_classify", self._single_classify, retry=RetryPolicy(max_attempts=3))
        graph.add_node("multi_classify", self._multi_classify)
        graph.add_node("get_titles", self._get_titles)
        graph.add_node("search_title", self._search_title)
        graph.add_node("multi_classify_long", self._multi_classify_long, retry=RetryPolicy(max_attempts=3))
        graph.add_node("multi_classify_short", self._multi_classify_short, retry=RetryPolicy(max_attempts=3))
        
        # æ·»åŠ è¾¹
        graph.add_edge(START, "get_app_description")
        graph.add_conditional_edges("get_app_description", self._router_by_multi_purpose)
        graph.add_edge("single_classify", END)
        graph.add_conditional_edges("multi_classify", self._router_by_duration_for_multi)
        graph.add_edge("multi_classify_short", END)
        graph.add_conditional_edges("get_titles", self._send_title)
        graph.add_edge("search_title", "multi_classify_long")
        graph.add_edge("multi_classify_long", END)
        
        self._app = graph.compile(store=self.store)
    
    def _record_token_usage(self, node_name: str, result: Any) -> None:
        """
        è®°å½• token ä½¿ç”¨æƒ…å†µåˆ° InMemoryStore
        
        Args:
            node_name: èŠ‚ç‚¹åç§°
            result: LLM invoke è¿”å›çš„ç»“æœ
        """
        raw_usage = result.response_metadata.get('token_usage', {})
        token_data = {
            'node': node_name,
            'input_tokens': raw_usage.get('input_tokens', 0),
            'output_tokens': raw_usage.get('output_tokens', 0),
            'total_tokens': raw_usage.get('total_tokens', 0),
            'search_count': raw_usage.get('plugins', {}).get('search', {}).get('count', 0)
        }
        
        # ä½¿ç”¨é€’å¢ key å­˜å‚¨
        self._token_counter += 1
        print(f"  ğŸ’° [{node_name}] tokens: {token_data['input_tokens']} in + {token_data['output_tokens']} out = {token_data['total_tokens']} total")
        print(token_data["search_count"])
        self.store.put(
            namespace=self.TOKEN_NAMESPACE,
            key=str(self._token_counter),
            value=token_data
        )
    
    def reset_token_usage(self) -> None:
        """é‡ç½® token ä½¿ç”¨ç»Ÿè®¡"""
        # åˆ é™¤æ‰€æœ‰ token è®°å½•
        items = list(self.store.search(self.TOKEN_NAMESPACE))
        for item in items:
            self.store.delete(namespace=self.TOKEN_NAMESPACE, key=item.key)
        self._token_counter = 0
    
    def get_token_summary(self) -> dict[str, dict]:
        """
        è·å– token ä½¿ç”¨æ±‡æ€»
        
        Returns:
            æŒ‰èŠ‚ç‚¹æ±‡æ€»çš„ token ä½¿ç”¨ç»Ÿè®¡
        """
        summary = {}
        items = list(self.store.search(self.TOKEN_NAMESPACE))
        
        for item in items:
            data = item.value
            node = data['node']
            if node not in summary:
                summary[node] = {
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'total_tokens': 0,
                    'search_count': 0
                }
            summary[node]['input_tokens'] += data['input_tokens']
            summary[node]['output_tokens'] += data['output_tokens']
            summary[node]['total_tokens'] += data['total_tokens']
            summary[node]['search_count'] += data['search_count']
        
        return summary
    
    def get_token_usage_list(self) -> list[dict]:
        """è·å–æ‰€æœ‰ token ä½¿ç”¨è®°å½•"""
        items = list(self.store.search(self.TOKEN_NAMESPACE))
        return [item.value for item in items]
    
    # ==================== è·¯ç”±å‡½æ•° ====================
    
    def _router_by_multi_purpose(self, state: classifyState) -> list[Send]:
        """è½¯ä»¶åˆ†ç±»è·¯ç”±ï¼šå•ç”¨é€”å’Œå¤šç”¨é€”åˆ†å¼€å¤„ç†"""
        single_state, multi_state = split_by_purpose(state)
        print(f"single_state:{single_state}")
        print(f"multi_state:{multi_state}")
        return [
            Send("single_classify", single_state),
            Send("multi_classify", multi_state)
        ]
    
    def _router_by_duration_for_multi(self, state: classifyState) -> list[Send]:
        """å¤šç”¨é€”åº”ç”¨æŒ‰æ—¶é•¿è·¯ç”±ï¼šçŸ­æ—¶é•¿å’Œé•¿æ—¶é•¿åˆ†å¼€å¤„ç†"""
        short_state, long_state = split_by_duartion(state)
        return [
            Send("multi_classify_short", short_state),
            Send("get_titles", long_state)
        ]
    
    def _send_title(self, input: SearchOutput) -> list[Send]:
        """ä¸ºæ¯ä¸ª id-title å¯¹åˆ›å»ºä¸€ä¸ª Send ä»»åŠ¡"""
        return [
            Send("search_title", {"id": item_id, "title": title})
            for item_id, title in input.input_data.items()
        ]
    
    # ==================== èŠ‚ç‚¹å‡½æ•° ====================
    
    def _get_app_description(self, state: classifyState) -> dict:
        """
        è·å–æ‰€æœ‰æ²¡æœ‰æè¿°çš„ app çš„æè¿°ä¿¡æ¯
        
        Args:
            state: classifyState å¯¹è±¡
            
        Returns:
            æ›´æ–°äº† app_registry çš„çŠ¶æ€å­—å…¸
        """
        # æ‰¾å‡ºæ‰€æœ‰æ²¡æœ‰æè¿°çš„ app
        app_to_search = []
        for app, app_info in state.app_registry.items():
            if app_info.description is None or app_info.description == "":
                title_sample = app_info.titles[0] if app_info.titles else ""
                app_to_search.append((app, title_sample))
        
        if not app_to_search:
            logger.info("æ‰€æœ‰ app éƒ½å·²æœ‰æè¿°ï¼Œè·³è¿‡æœç´¢")
            return {}
        
        logger.info(f"éœ€è¦æœç´¢æè¿°çš„ app: {[app for app, _ in app_to_search]}")
        
        # é¡ºåºæœç´¢æ¯ä¸ª app çš„æè¿°
        app_descriptions = {}
        
        system_message = SystemMessage(content="""
        ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶ç¨‹åºè¯†åˆ«ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€šè¿‡ web æœç´¢è¯†åˆ«è½¯ä»¶åº”ç”¨ç¨‹åºï¼Œå¹¶æä¾›å‡†ç¡®ã€ç²¾ç‚¼çš„æè¿°ã€‚
        **è¾“å…¥è¯´æ˜ï¼š**
        - è¾“å…¥è½¯ä»¶åç§°æˆ–ç¨‹åºåç§°ä¸çª—å£title
        **è¾“å‡ºè¦æ±‚ï¼š**
        - è½¯ä»¶æè¿°(ä¸è¶…è¿‡20è¯):ä»¥webæœç´¢ä¸ºä¸»,titleä¿¡æ¯ä¸ºè¾…
        - è¿”å›è½¯ä»¶æè¿°
        - å¦‚æœæœç´¢åä»æ— æ³•ç¡®å®šï¼Œè¿”å› None
        """)
        
        for app, title in app_to_search:
            try:
                user_message = HumanMessage(content=f"""è½¯ä»¶åç§°:{app} title:{title}""")
                messages = [system_message, user_message]
                
                result = self.chat_model.invoke(messages)
                self._record_token_usage("get_app_description", result)
                
                app_descriptions[app] = result.content
                logger.info(f"å·²è·å– {app} çš„æè¿°: {result.content[:50]}...")
                
            except Exception as e:
                logger.error(f"æœç´¢ {app} æè¿°å¤±è´¥: {e}")
                app_descriptions[app] = None
        
        # æ›´æ–° state.app_registry
        for app_name, description in app_descriptions.items():
            if app_name in state.app_registry:
                state.app_registry[app_name].description = description
        
        return {"app_registry": state.app_registry}
    
    def _single_classify(self, state: classifyState) -> dict:
        """å•ç”¨é€” app åˆ†ç±»ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
        goal = format_goals_for_prompt(state.goal)
        category_tree = format_category_tree_for_prompt(state.category_tree)
        
        system_message = SystemMessage(content=f"""
        # ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶åˆ†ç±»ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®è½¯ä»¶åç§°,æè¿°,å°†è½¯ä»¶è¿›è¡Œåˆ†ç±»,åˆ†ç±»æœ‰categoryå’Œsub_categoryä¸¤çº§åˆ†ç±»ã€‚
        # åˆ†ç±»ç±»åˆ«
        {category_tree}
        # ç”¨æˆ·ç›®æ ‡
        {goal}
        # åˆ†ç±»è§„åˆ™
        1. å¯¹äºappä¸goalé«˜åº¦ç›¸å…³çš„æ¡ç›®,ä½¿ç”¨goalçš„åˆ†ç±»ç±»åˆ«,å¹¶å…³è”goal,link_to_goal = goal;å¦åˆ™link_to_goal = null
        2. å¯¹äºå•ç”¨é€”,ä¾æ®app_descriptionè¿›è¡Œåˆ†ç±»,è‹¥æ— æ³•åˆ†ç±»,åˆ™åˆ†ç±»ä¸ºnull
        3. è‹¥categoryæœ‰åˆ†ç±»è€Œsub_categoryæ— æ³•åˆ†ç±»,åˆ™sub_category = null
        # è¾“å‡ºæ ¼å¼ä¸ºjson,keyä¸ºå¯¹äºæ•°æ®çš„id,valueä¸ºä¸€ä¸ªlist[category,sub_category,link_to_goal]
        {{
            id:[category,sub_category,link_to_goal]
        }}
        ç¤ºä¾‹:
        {{
            "1": ["å·¥ä½œ/å­¦ä¹ ", "ç¼–ç¨‹", "å®ŒæˆLifeWatch-AIé¡¹ç›®å¼€å‘"],
            "2": ["å¨±ä¹", "çœ‹ç”µè§†", null]
        }}
        æ³¨æ„ï¼š
        - valueå¿…é¡»æ˜¯åˆ—è¡¨ï¼ŒåŒ…å«ä¸‰ä¸ªå…ƒç´  [category, sub_category, link_to_goal]
        - æ— å€¼æ—¶ä½¿ç”¨ null
        - keyå¿…é¡»æ˜¯idï¼Œä¸æ˜¯appåç§°
        """)
        
        # è·å–å•ç”¨é€”çš„ log_item
        single_purpose_items = [
            item for item in state.log_items
            if not state.app_registry[item.app].is_multipurpose
        ]
        
        if not single_purpose_items:
            logger.info("æ²¡æœ‰å•ç”¨é€”åº”ç”¨éœ€è¦åˆ†ç±»")
            return {}
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(single_purpose_items), self.MAX_LOG_ITEMS):
            batch = single_purpose_items[i:i + self.MAX_LOG_ITEMS]
            batch_num = i // self.MAX_LOG_ITEMS + 1
            logger.info(f"single_classify å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼Œå…± {len(batch)} æ¡è®°å½•")
            
            app_content = format_log_items_table(
                batch,
                fields=["id", "app", "title"],
                app_registry=state.app_registry,
                group_by_app=True,
                show_app_description=True
            )
            print(f"_single_classify:{app_content}")
            human_message = HumanMessage(content=app_content)
            messages = [system_message, human_message]
            
            results = self.chat_model.invoke(messages)
            self._record_token_usage("single_classify", results)
            
            logger.debug(f"LLM åŸå§‹å“åº” (æ‰¹æ¬¡ {batch_num}): {results.content}")
            
            clean_content = extract_json_from_response(results.content)
            classification_result = json.loads(clean_content)
            logger.info(f"single_classify æ‰¹æ¬¡ {batch_num} æˆåŠŸè·å–åˆ†ç±»ç»“æœ")
            
            state = parse_classification_result(state, classification_result, "single_classify")
        
        return {"result_items": state.log_items}
    
    def _multi_classify(self, state: classifyState) -> classifyState:
        """å¤šç”¨é€”åˆ†ç±»ç©ºèŠ‚ç‚¹ï¼Œåç»­æ¥ä¸Šå¤šåˆ†ç±»è·¯ç”±"""
        return {}
    
    def _multi_classify_short(self, state: classifyState) -> dict:
        """çŸ­æ—¶é•¿å¤šç”¨é€”åˆ†ç±»ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
        category_tree = format_category_tree_for_prompt(state.category_tree)
        goal = format_goals_for_prompt(state.goal)
        
        system_message = SystemMessage(content=f"""
        ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·è¡Œä¸ºåˆ†æä¸“å®¶,ä½ éœ€è¦ä¾æ®ç”¨æˆ·çš„æµè§ˆçš„ç½‘é¡µtitleå¯¹ç”¨æˆ·çš„è¡Œä¸ºè¿›è¡Œåˆ†ç±»
        # ç±»åˆ«:
        {category_tree}
        # ç”¨æˆ·ç›®æ ‡:
        {goal}
        # åˆ†ç±»è§„åˆ™:
        1. å¯¹äºtitleä¸goalé«˜åº¦ç›¸å…³çš„æ¡ç›®,ä½¿ç”¨goalçš„åˆ†ç±»ç±»åˆ«,å¹¶å…³è”goal,link_to_goal = goal;å¦åˆ™link_to_goal = null
        2. æå–å‡ºtitleä¸­çš„ç½‘ç«™åç§°å’Œç½‘ç«™æ ‡é¢˜,é€šè¿‡è¿™ä¸¤ä¸ªè¦ç´ è¿›è¡Œåˆ†ç±»
        3. ç±»åˆ«æœ‰ä¸¤ä¸ªå±‚çº§category->sub_category,åˆ†ç±»ç»“æœsub_categoryè¦å±äºcategoryã€‚å½“æ²¡æœ‰åŒ¹é…é¡¹æ—¶,åˆ†ç±»ä¸ºnull
        # è¾“å‡ºæ ¼å¼ä¸ºjson,keyä¸ºå¯¹äºæ•°æ®çš„id,valueä¸ºä¸€ä¸ªlist[category,sub_category,link_to_goal]
        {{
            id:[category,sub_category,link_to_goal]
        }}
        ç¤ºä¾‹:
        {{
            "1": ["å·¥ä½œ/å­¦ä¹ ", "ç¼–ç¨‹", "å®ŒæˆLifeWatch-AIé¡¹ç›®å¼€å‘"],
            "2": ["å¨±ä¹", "çœ‹ç”µè§†", null]
        }}
        """)
        
        if not state.log_items:
            logger.info("æ²¡æœ‰çŸ­æ—¶é•¿å¤šç”¨é€”åº”ç”¨éœ€è¦åˆ†ç±»")
            return {}
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(state.log_items), self.MAX_LOG_ITEMS):
            batch = state.log_items[i:i + self.MAX_LOG_ITEMS]
            batch_num = i // self.MAX_LOG_ITEMS + 1
            logger.info(f"multi_classify_short å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼Œå…± {len(batch)} æ¡è®°å½•")
            
            items = format_log_items_table(
                batch,
                fields=["id", "app", "title", "title_analysis"]
            )
            print(f"_multi_classify_short{items}")
            human_message = HumanMessage(content=f"""å¯¹ä¸‹é¢çš„æ•°æ®è¿›è¡Œåˆ†ç±»:\n{items}""")
            messages = [system_message, human_message]
            
            result = self.chat_model.invoke(messages)
            self._record_token_usage("multi_classify_short", result)
            
            logger.debug(f"LLM åŸå§‹å“åº” (æ‰¹æ¬¡ {batch_num}): {result.content}")
            
            clean_content = extract_json_from_response(result.content)
            classification_result = json.loads(clean_content)
            logger.info(f"multi_classify_short æ‰¹æ¬¡ {batch_num} æˆåŠŸè·å–åˆ†ç±»ç»“æœ")
            
            state = parse_classification_result(state, classification_result, "multi_classify_short")
        
        return {"result_items": state.log_items}
    
    def _get_titles(self, state: classifyState) -> dict:
        """è·å– title å­—å…¸ç”¨äºå¹¶å‘æœç´¢"""
        title_dict = {}
        for item in state.log_items:
            if item.title:
                title_dict[item.id] = item.title
        return {"input_data": title_dict}
    
    def _search_title(self, input: dict) -> dict:
        """æœç´¢å¹¶åˆ†æå•ä¸ª title"""
        item_id = input["id"]
        title = input["title"]
        
        system_message = SystemMessage(content="""
        ä½ æ˜¯ä¸€ä¸ªé€šè¿‡ç½‘ç»œæœç´¢åˆ†æçš„åŠ©æ‰‹,ä¾æ®ç½‘ç»œæœç´¢ç»“æœå’Œtitleåˆ†æç”¨æˆ·çš„æ´»åŠ¨ï¼Œè¦æ±‚ç»“æœåœ¨50å­—ä»¥å†…
        # è¾“å‡ºæ ¼å¼:str å†…å®¹ä¸º:ç”¨æˆ·æ´»åŠ¨
        """)
        human_message = HumanMessage(content=f"""æœç´¢å¹¶åˆ†æ{title}""")
        messages = [system_message, human_message]
        
        try:
            result = self.chat_model.invoke(messages)
            self._record_token_usage("search_title", result)
            
            logger.debug(f"search_title å“åº”: {result.content}")
            title_analysis_result = result.content
        except Exception as e:
            logger.error(f"search_title {title} æ‰§è¡Œå¤±è´¥, é”™è¯¯: {e}")
            title_analysis_result = None
        
        return {
            "title_analysis_results": [(item_id, title_analysis_result)]
        }
    
    def _multi_classify_long(self, state: classifyState) -> dict:
        """é•¿æ—¶é•¿å¤šç”¨é€”åˆ†ç±»ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
        goal = format_goals_for_prompt(state.goal)
        category_tree = format_category_tree_for_prompt(state.category_tree)
        
        system_message = SystemMessage(content=f"""
        ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·è¡Œä¸ºåˆ†ç±»ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç½‘é¡µæ ‡é¢˜(Title)å’Œæ ‡é¢˜åˆ†æ(Title Analysis)å¯¹ç”¨æˆ·çš„è¡Œä¸ºè¿›è¡Œåˆ†ç±»ã€‚
        
        # åˆ†ç±»ç±»åˆ«
        {category_tree}
        
        # ç”¨æˆ·ç›®æ ‡
        {goal}
        
        # åˆ†ç±»è§„åˆ™
        1. å¯¹äºä¸goalé«˜åº¦ç›¸å…³çš„æ¡ç›®,ä½¿ç”¨goalçš„åˆ†ç±»ç±»åˆ«,å¹¶å…³è”goal,link_to_goal = goal;å¦åˆ™link_to_goal = null
        2. ä¸»è¦ä¾æ®Title Analysisæ¥ç†è§£ç”¨æˆ·çš„æ´»åŠ¨å†…å®¹,ç»“åˆTitleè¿›è¡Œåˆ†ç±»
        3. ç±»åˆ«æœ‰ä¸¤ä¸ªå±‚çº§category->sub_category,åˆ†ç±»ç»“æœsub_categoryè¦å±äºcategory
        4. è‹¥categoryæœ‰åˆ†ç±»è€Œsub_categoryæ— æ³•åˆ†ç±»,åˆ™sub_category = null
        5. è‹¥æ— æ³•åˆ†ç±»,åˆ™åˆ†ç±»ä¸ºnull
        
        # è¾“å‡ºæ ¼å¼ä¸ºjson,keyä¸ºæ•°æ®çš„id,valueä¸ºä¸€ä¸ªlist[category,sub_category,link_to_goal]
        {{
            "id":[category,sub_category,link_to_goal]
        }}

        ç¤ºä¾‹ï¼š
        {{
            "1": ["å·¥ä½œ/å­¦ä¹ ", "ç¼–ç¨‹", "å®ŒæˆLifeWatch-AIé¡¹ç›®å¼€å‘"],
            "2": ["å¨±ä¹", "çœ‹ç”µè§†", null]
        }}
        æ³¨æ„ï¼š
        - valueå¿…é¡»æ˜¯åˆ—è¡¨ï¼ŒåŒ…å«ä¸‰ä¸ªå…ƒç´  [category, sub_category, link_to_goal]
        - æ— å€¼æ—¶ä½¿ç”¨ null
        - keyå¿…é¡»æ˜¯idï¼Œä¸æ˜¯appåç§°
        """)
        
        if not state.log_items:
            logger.info("æ²¡æœ‰é•¿æ—¶é•¿å¤šç”¨é€”åº”ç”¨éœ€è¦åˆ†ç±»")
            return {}
        
        # ä» title_analysis_results æ„å»º id -> analysis çš„æ˜ å°„
        analysis_map = {}
        if state.title_analysis_results:
            for item_id, analysis in state.title_analysis_results:
                analysis_map[item_id] = analysis
        
        # æ›´æ–° log_items çš„ title_analysis å­—æ®µ
        for item in state.log_items:
            if item.id in analysis_map:
                item.title_analysis = analysis_map[item.id]
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(state.log_items), self.MAX_LOG_ITEMS):
            batch = state.log_items[i:i + self.MAX_LOG_ITEMS]
            batch_num = i // self.MAX_LOG_ITEMS + 1
            logger.info(f"multi_classify_long å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼Œå…± {len(batch)} æ¡è®°å½•")
            
            items = format_log_items_table(
                batch,
                fields=["id", "app", "title", "title_analysis"]
            )
            print(f"multi_classify_long:{items}")
            human_message = HumanMessage(content=f"""
            è¯·å¯¹ä»¥ä¸‹ç”¨æˆ·è¡Œä¸ºæ•°æ®è¿›è¡Œåˆ†ç±»ï¼š
            {items}
            """)
            print(f"_multi_classify_long{items}")
            messages = [system_message, human_message]
            
            result = self.chat_model.invoke(messages)
            print(result)
            self._record_token_usage("multi_classify_long", result)
            
            logger.debug(f"LLM åŸå§‹å“åº” (æ‰¹æ¬¡ {batch_num}): {result.content}")
            
            clean_content = extract_json_from_response(result.content)
            classification_result = json.loads(clean_content)
            logger.info(f"multi_classify_long æ‰¹æ¬¡ {batch_num} æˆåŠŸè·å–åˆ†ç±»ç»“æœ")
            
            state = parse_classification_result(state, classification_result, "multi_classify_long")
        
        return {"result_items": state.log_items}
    
    # ==================== å…¬å…±æ¥å£ ====================
    
    def run(self, state: classifyState) -> dict:
        """
        æ‰§è¡Œåˆ†ç±»æµç¨‹
        
        Args:
            state: åˆå§‹åˆ†ç±»çŠ¶æ€
            
        Returns:
            åˆ†ç±»ç»“æœå­—å…¸
        """
        self.reset_token_usage()
        config = {"configurable": {"thread_id": "thread-123"}}
        return self._app.invoke(state,config = config)
    
    def print_token_summary(self) -> None:
        """æ‰“å° token ä½¿ç”¨ç»Ÿè®¡"""
        token_summary = self.get_token_summary()
        if not token_summary:
            print("æš‚æ—  token ä½¿ç”¨è®°å½•")
            return
        
        print("\nã€Token ä½¿ç”¨ç»Ÿè®¡ã€‘")
        total_tokens = 0
        total_search_count = 0
        
        for node_name, usage in token_summary.items():
            print(f"\n  {node_name}:")
            print(f"    - Input Tokens:  {usage.get('input_tokens', 0):,}")
            print(f"    - Output Tokens: {usage.get('output_tokens', 0):,}")
            print(f"    - Total Tokens:  {usage.get('total_tokens', 0):,}")
            print(f"    - Search Count:  {usage.get('search_count', 0)}")
            total_tokens += usage.get('total_tokens', 0)
            total_search_count += usage.get('search_count', 0)
        
        print(f"\n  æ€»è®¡ Token ä½¿ç”¨: {total_tokens:,}")
        print(f"  æ€»è®¡æœç´¢æ¬¡æ•°: {total_search_count}")
        print(f"  API è°ƒç”¨æ¬¡æ•°: {len(self.get_token_usage_list())}")


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    from lifewatch.llm.llm_classify.classify.data_loader import (
        get_real_data,
        filter_by_duration,
        deduplicate_log_items
    )
    
    def get_state(hours: int = 36) -> classifyState:
        state = get_real_data(hours=hours)
        state = filter_by_duration(state, min_duration=60)
        state = deduplicate_log_items(state)
        
        print(f"\nå»é‡åçš„æ—¥å¿—ï¼ˆå‰10æ¡ï¼‰:")
        for item in state.log_items[:10]:
            multipurpose = "å¤šç”¨é€”" if state.app_registry[item.app].is_multipurpose else "å•ç”¨é€”"
            print(f"  {item.app} ({multipurpose}) | {item.title} | {item.duration}s")
        
        print(f"\næµ‹è¯•è¿‡æ»¤åŠŸèƒ½ï¼ˆåªä¿ç•™ duration >= 60 ç§’çš„è®°å½•ï¼‰:")
        print(f"  - è¿‡æ»¤å log_items: {len(state.log_items)} æ¡")
        print(f"  - è¿‡æ»¤å app_registry: {len(state.app_registry)} ä¸ªåº”ç”¨")
        return state
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = LLMClassify()
    
    # è·å–æµ‹è¯•æ•°æ®
    state = get_state(hours=8)
    print(state.app_registry)
    input_items_len = len(state.log_items)
    
    # æ‰§è¡Œåˆ†ç±»
    output = classifier.run(state)
    print(output)
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 80)
    print("åˆ†ç±»ç»“æœæ±‡æ€»")
    print("=" * 80)
    
    # è¾“å‡º token ä½¿ç”¨æƒ…å†µ
    classifier.print_token_summary()
    
    # è¾“å‡ºåˆ†ç±»ç»“æœ
    if "result_items" in output:
        print("\nã€åˆ†ç±»ç»“æœã€‘")
        print(f"  å…±åˆ†ç±» {len(output['result_items'])} æ¡è®°å½•\n")
        
        for item in output["result_items"]:
            print(f"  ID: {item.id}")
            print(f"    åº”ç”¨: {item.app}")
            if item.title:
                print(f"    æ ‡é¢˜: {item.title[:50]}{'...' if len(item.title) > 50 else ''}")
            print(f"    åˆ†ç±»: {item.category or 'N/A'} -> {item.sub_category or 'N/A'}")
            print(f"    å…³è”ç›®æ ‡: {item.link_to_goal or 'N/A'}")
            print(f"    æ—¶é•¿: {item.duration}s")
            print(f"{item.title_analysis}")
            print()
    
    print("=" * 80)
    print(f"è¾“å…¥ä¸ªæ•°: {input_items_len}")
